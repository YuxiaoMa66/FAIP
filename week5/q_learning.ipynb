{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment for FAIP lecture \"Reinforcement Learning\"\n",
    "\n",
    "In this assignment you will implement tabular Q-learning (from the 2nd video) and test it in the MountainCar environment (from the 1st video). To pass this assignment, you must make a serious attempt to solve all 7 tasks (your answer does not have to be correct, but you must show you tried) and achieve at least 7 of the 11 possible points. You should not spend much more than 4-5 hours on the assignment.\n",
    "\n",
    "### Tasks you must solve:\n",
    "- [Task 1: identify the MDP (1 point)](#mdp)\n",
    "- [Task 2: familiarize yourself with Q-learning (1 point)](#value)\n",
    "- [Task 3: implement Q-learning (2 points)](#TD)\n",
    "- [Task 4: implement the learning loop of RL (2 points)](#RL)\n",
    "- [Task 5: test your implementation (1 point)](#test)\n",
    "- [Task 6: resolution trade-off (2 points)](#res)\n",
    "- [Task 7: optimistic exploration (2 points)](#exp)\n",
    "\n",
    "### Literature\n",
    "- Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018, [online version](http://incompleteideas.net/book/the-book-2nd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class`MountainCar` specifies a variant of the classical `MountainCar-v0` environment, where the reward is 1 if the goal on the top of the right hill is reached and 0 otherwise. You can start a new episode with `reset()` and execute an `action` with `step(action)`.\n",
    "![An animation of the MountainCar environment in action](https://user-images.githubusercontent.com/10624937/42135605-ba0e5f2c-7d12-11e8-9578-86d74e0976f8.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class MountainCar (gym.Wrapper):\n",
    "    def __init__(self):\n",
    "        super().__init__(env=gym.make('MountainCar-v0'))\n",
    "        self._elapsed_steps = self.env._elapsed_steps\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" Starts a new episode and returns the first state. \"\"\"\n",
    "        self._elapsed_steps = 0\n",
    "        return self.env.reset()[0]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\" Executes an `action` in the current state. Returns the `next_state`, the received `reward`, \n",
    "            a boolean `done` indicating whether the episode is finished (terminal or timeout), and \n",
    "            a boolean `terminal` indicating whether a `done` episode ended in a terminal state.\"\"\"\n",
    "        next_state, _, terminal, truncated, _ = super().step(action)\n",
    "        self._elapsed_steps += 1\n",
    "        done = truncated or terminal\n",
    "        reward = 1 if terminal else 0\n",
    "        return next_state, reward, done, terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: identify the MDP (1 point) <a id=mdp></a>\n",
    "\n",
    "The reinforcement learning algorithm interacts with the above class `MountainCar` only through 2 methods. Look up the definition of a Markov Decision Process (MDP) and indicate which inputs/outputs of `reset()` and `step()` are corresponding to which elements of the MDP, and from/by which distribution/function they are drawn/determined.\n",
    "\n",
    "*See Sections 3.1 to 3.3 in Suttan and Barto (2018) for more information on MDP, reward and return.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ MDP = \\langle S, A, \\rho, P, r \\rangle $\n",
    "- `reset()`:\n",
    "    - output is initial state $s_0 \\sim \\rho, s_0 \\in S$\n",
    "- `step()`:\n",
    "    - input `action` ***TODO***\n",
    "    - output `nexts state` ***TODO***\n",
    "    - output `reward` ***TODO***\n",
    "    - output `done` ***TODO***\n",
    "    - output `terminal` ***TODO***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: familiarize yourself with Q-learning (1 point) <a id=value></a>\n",
    "\n",
    "Read the lecture slides of video 2 again (and watch the video if it helps). Try to understand the \"temporal difference\" update rule of tabular Q-learning. You will use the class `ValueFunction` below later to store a table of Q-values. Make sure you know how to read and write Q-values using that class before you continue.\n",
    "\n",
    "*See Section 6.5 in Sutton and Barto (2018) for more details on Q-learning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class ValueFunction:\n",
    "    \"\"\" Defines a tabular Q-value function for a given 'env'ironment and\n",
    "        optionally a resolution (int or Iterable of int) and\n",
    "        optionally some 'state_bounds' (list of 2-tuples, one for each state dimension).\n",
    "        \n",
    "        For example, defining 'q = ValueFunction(env)' allows to: \n",
    "        set values with \n",
    "            'q[state] = [value_1, ..., value_num_actions]' or \n",
    "            'q[state, action] = single_value', and \n",
    "        get values with \n",
    "            'q[state]' returns an array with the values of all actions or\n",
    "            'q[state, action]' returns the value of the given 'action'.\"\"\"\n",
    "    def __init__(self, env, resolution=50, state_bounds=None, init_value=0.0):\n",
    "        # derive the action space\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete), \"The environment must have discrete actions.\"\n",
    "        self.num_actions = env.action_space.n\n",
    "        # derive the the state space\n",
    "        if state_bounds is None:\n",
    "            self.bounds = [(l, h) for l,h in zip(env.observation_space.low, env.observation_space.high)]\n",
    "        else:\n",
    "            self.bounds = state_bounds\n",
    "        self.resolution = resolution if isinstance(resolution, Iterable) else [resolution for _ in self.bounds]\n",
    "        assert len(self.resolution) == len(self.bounds), \\\n",
    "                \"The resolution and state bounds must be consistent with the given environment.\"\n",
    "        # define other properties\n",
    "        self.eps = 1E-7\n",
    "        # define the lookup table for the values\n",
    "        self.shape = (*self.resolution, self.num_actions)\n",
    "        self.val = np.ones(self.shape) * (init_value)  # + self.eps)\n",
    "        \n",
    "    def state_bin(self, state):\n",
    "        \"\"\" Find the correct bin in 'self.count' for one state. \"\"\"\n",
    "        return tuple([int((x - l) / (h - l + self.eps) * r) for x, (l, h), r in zip(state, self.bounds, self.resolution)])\n",
    "        \n",
    "    def __setitem__(self, key, newvalue):\n",
    "        assert isinstance(key, Iterable) and len(key) > 0, \\\n",
    "                    \"x[state] = actions or x[state, action] = value\"\n",
    "        if isinstance(key[0], Iterable) and len(key) > 1:\n",
    "            assert key[1] >= 0 and key[1] < self.num_actions, \\\n",
    "                    \"illegal action %u, must be between 0 and %u\" % (key[1], self.num_actions-1)\n",
    "            self.val[self.state_bin(key[0])][key[1]] = newvalue\n",
    "        else:\n",
    "            assert len(newvalue) == self.num_actions, \\\n",
    "                    \"wrong size %u of value-vector, must be %u\" % (len(newvalue), self.num_actions)\n",
    "            self.val[self.state_bin(key)] = newvalue\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        assert isinstance(key, Iterable) and len(key) > 0\n",
    "        if isinstance(key[0], Iterable) and len(key) > 1:\n",
    "            return self.val[self.state_bin(key[0])][key[1]]\n",
    "        else:\n",
    "            return self.val[self.state_bin(key)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that you can use `ValueFunction`, fill in the missing 4 lines below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ValueFunction(env=MountainCar(), resolution=5, init_value=1)\n",
    "# TODO: overwrite the Q-value of state (0.5,0) and action 1 with the value 2\n",
    "\n",
    "# TODO: print all Q-values in state (0.5,0) \n",
    "\n",
    "# TODO: print the Q-values of action 1 in state(0.5, 0.05)\n",
    "\n",
    "# TODO: print all Q-values of state (0.4, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain in the field below why the Q-values of states $s=(0.5, 0)$ and $s'=(0.4, 0)$ are identical, even though $s'$ has never been changed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation:*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: implement Q-learning (2 points) <a id=TD></a>\n",
    "Look up the lecture slide \"tabular Q-learning\" and implement the temporal difference update rule in the method `learn` of the given class `QLearningAgent`. The method receives a starting `state`, an `action` that is executed in `state`, a `next_state` after execution of `action`, a received `reward` for that transition and a boolean `terminal` that is `True` if `next_state` is a terminal state (i.e. the goal of MountainCar). For full points, make sure the update is correct in `terminal` states, i.e., after the transition into `next_state`, no further rewards are received and accounted for in the learned Q-values.\n",
    "\n",
    "*See Section 6.5 in Sutton and Barto (2018) for more details on Q-learning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.5, gamma=0.99, resolution=50, init_value=1.0):\n",
    "        self.q = ValueFunction(env, resolution=resolution, init_value=init_value)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def choose(self, state):\n",
    "        return np.argmax(self.q[state])\n",
    "    \n",
    "    def learn(self, state, action, next_state, reward, terminal):\n",
    "        # TODO: update the Q-values in `self.q` here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: implement the learning loop of RL (2 points) <a id=RL></a>\n",
    "\n",
    "Extend the method `train()` of the following class `RLExperiment`, which is supposed to learn the `MountainCar` environment using the above `QLearningAgent`. The class also manages experiment results and plots learning curves, q-values and policies in regular intervals. Due to its complexity, it is not necessary that you fully understand it, you must just implement the `train_episode()` method (no other methods need to be modified), similar to the lecture slides.\n",
    "\n",
    "*See Section 3.1 in Sutton and Barto (2018) for more details on the agent-environment interface.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "class RLExperiment:\n",
    "    \"\"\" Main class that initializes, runs and plots an RL experiment. \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.5, gamma=0.95, resolution=50, init_value=1, plot_interval=50):\n",
    "        self.plot_results = plot_interval > 0\n",
    "        self.plot_interval = plot_interval\n",
    "        self.env = MountainCar()\n",
    "        self.max_episode_length = self.env.spec.max_episode_steps\n",
    "        self.init_value = init_value\n",
    "        self.agent = QLearningAgent(self.env, alpha=alpha, gamma=gamma, resolution=resolution, init_value=init_value)\n",
    "        self.episode_returns = []\n",
    "        self.env_steps = []\n",
    "        \n",
    "    def _plot_states(self, image, colormap='jet', ticks=None):\n",
    "        \"\"\" Internal method that plots a colorplot of some image. \n",
    "             You do not need to understand (or change) the details of this method. \"\"\"\n",
    "        # colorplot of 'image' and a white finish line\n",
    "        pl.imshow(np.transpose(np.flip(image, axis=1), (1, 0)), cmap=colormap)\n",
    "        idx = self.agent.q.state_bin((0.5, 0))\n",
    "        pl.plot([idx[0]-0.5, idx[0]-0.5], [0, image.shape[1]-1], 'w:')\n",
    "        # set the axis\n",
    "        idx = self.agent.q.state_bin((-0.5, 0))\n",
    "        pl.xticks([-0.5, idx[0], self.agent.q.val.shape[0] - 0.5], [-1.2, -0.5, 0.6])\n",
    "        pl.xlabel('position')\n",
    "        pl.yticks([-0.5, idx[1], self.agent.q.val.shape[1] - 0.5], [0.07, 0, -0.07])\n",
    "        pl.ylabel('velocity')\n",
    "        # add a colorbar with the given 'colormap' (and eventually with the defined 'ticks')\n",
    "        if ticks is None:\n",
    "            pl.colorbar()\n",
    "        else:\n",
    "            cbar = pl.colorbar(ticks=[i - 1 for i in range(len(ticks))])\n",
    "            cbar.ax.set_yticklabels(ticks)  # vertically oriented colorbar\n",
    "\n",
    "    def _plot_training(self, update=False):\n",
    "        \"\"\" Plots logged training results. Use \"update=True\" if the plot is continuously updated\n",
    "            or use \"update=False\" if this is the final call (otherwise there will be double plotting). \n",
    "            You do not need to understand (or change) the details of this method. \"\"\" \n",
    "        # Smooth curves\n",
    "        window = max(int(len(self.episode_returns) / 50), 10)\n",
    "        if len(self.episode_returns) < window + 2: return\n",
    "        returns = np.convolve(self.episode_returns, np.ones(window)/window, 'valid')\n",
    "        env_steps = np.convolve(self.env_steps, np.ones(window)/window, 'valid')\n",
    "        # Create plot\n",
    "        colors = ['b', 'g', 'r']\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(16, 4)\n",
    "        plt.clf()\n",
    "        # Plot the returns in the left subplot\n",
    "        pl.subplot(1, 3, 1)\n",
    "        pl.plot(env_steps, returns, colors[0])\n",
    "        pl.xlabel('environment steps')\n",
    "        pl.ylabel('averaged episode return')\n",
    "        # Plot the value function in the second plot\n",
    "        pl.subplot(1, 3, 2)\n",
    "        self._plot_states(self.agent.q.val.max(axis=-1))\n",
    "        pl.title('values')\n",
    "        # Plot the policy in the third plot\n",
    "        pl.subplot(1, 3, 3)\n",
    "        policy = np.argmax(self.agent.q.val, axis=-1)\n",
    "        policy[(self.agent.q.val == self.init_value).all(axis=-1)] = -1\n",
    "        self._plot_states(policy, colormap='gnuplot', ticks=['unvisited', 'left', 'none', 'right'])\n",
    "        pl.title('policy')\n",
    "        # dynamic plot update\n",
    "        display.clear_output(wait=True)\n",
    "        if update:\n",
    "            display.display(pl.gcf())\n",
    "            \n",
    "    def close(self):\n",
    "        \"\"\" Closes the environment (free resources). Do not change this method. \"\"\"\n",
    "        self.env.close()\n",
    "        \n",
    "    def train(self, num_episodes=float('inf'), max_env_steps=float('inf')):\n",
    "        \"\"\" Trains a RL agent for `num_episodes` or until `max_env_steps`. \n",
    "            Do not change this method! \"\"\"\n",
    "        env_steps, episode = 0, 0\n",
    "        while episode < num_episodes and env_steps < max_env_steps:\n",
    "            episode_return = self.train_episode()\n",
    "            # increase counters\n",
    "            env_steps += self.env._elapsed_steps\n",
    "            episode += 1\n",
    "            # remember results\n",
    "            self.episode_returns.append(episode_return)\n",
    "            self.env_steps.append(env_steps)\n",
    "            # plot the loss, values and policy\n",
    "            if self.plot_results and (episode + 1) % self.plot_interval == 0: \n",
    "                self._plot_training(update=True)\n",
    "        if self.plot_results:\n",
    "            self._plot_training(update=False)\n",
    "            \n",
    "    def train_episode(self):\n",
    "        \"\"\" Run and train the agent with one episode. This method must be completed by you! \"\"\"\n",
    "        # TODO: start a new episode and run it until the environemnt returns `done==True`.\n",
    "        # - update the Q-learning agent `self.agent` in every step\n",
    "        # - the function must return the sum of all experienced rewards during this episode\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: test your implementation (1 point)  <a id=test></a>\n",
    "\n",
    "Running the following code segment runs Q-learning for 1 million environment steps using the following parameters:\n",
    "- learning rate `alpha=0.5`\n",
    "- discount factor `gamma=0.99`\n",
    "- discretization `resolution=31`\n",
    "- table initialization `init_val=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ex \u001b[38;5;241m=\u001b[39m RLExperiment(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m, init_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m ex\u001b[38;5;241m.\u001b[39mtrain(max_env_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1E6\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 83\u001b[0m, in \u001b[0;36mRLExperiment.train\u001b[1;34m(self, num_episodes, max_env_steps)\u001b[0m\n\u001b[0;32m     81\u001b[0m episode_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_episode()\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# increase counters\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m env_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39m_elapsed_steps\n\u001b[0;32m     84\u001b[0m episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# remember results\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "ex = RLExperiment(alpha=0.5, gamma=0.99, resolution=31, init_value=1)\n",
    "ex.train(max_env_steps=1E6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: resolution trade-off (2 points) <a id=res></a>\n",
    "\n",
    "Play around with the `resolution` parameter of `RLExperiment`. What is the smallest resolution with which you can still learn the task? Does this resolution learn in fewer or more environmental steps? Is the final performance (episode return) the same? Show the final plots of ***at least 3*** examples that demonstrate your findings.\n",
    "Also describe your observations, conclusions and answers to the above questions (in few words) in the field below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: optimistic exploration (2 points) <a id=exp></a>\n",
    "\n",
    "In this task you will investigate the exploration behavior of the Q-learning agent. Frist, run the configuration of Task 4 *without optimistic initialization*, that is, with `init_value=0`.\n",
    "\n",
    "*See Section 2.6 in Sutton and Barto (2018) for more details on optimistic intialization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = RLExperiment(alpha=0.5, gamma=0.99, resolution=31, init_value=0)\n",
    "ex.train(max_env_steps=1E6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above agent does not learn, because it does not explore the environment and therefore Q-learning does not converge to the optimal value function. Next, play around with the initial table value. Starting with `init_value=0.1`, how well does the Q-value agent learn for varying `init_value`? Is the final performance the same? Is that performance learned in fewer environmental steps? Show the final plots of ***at least 3*** expamples.\n",
    "Also describe your observations, conclusions and answers to the above questions (in few words) in the field below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
